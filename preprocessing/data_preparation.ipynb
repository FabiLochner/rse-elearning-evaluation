{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97009b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import relevant libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pdf_text_extraction import (\n",
    "        extract_text_from_pdf,\n",
    "        extract_main_content,\n",
    "        extract_references,\n",
    "        process_pdf_with_metadata,\n",
    "        process_pdf_without_metadata\n",
    "    )\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress openpyxl extension warnings (common with Excel files from different sources)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb07e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full proceedings to exclude: 20\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1) Configuration\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "OUTPUT_DIR = DATA_DIR  # CSV backup saved to data folder\n",
    "\n",
    "\n",
    "# LNI to year mapping (necesseary for files without metadata or for files with metadataa, in casee the \"dc.date.issued\" column in the metadata excel files has no values)\n",
    "LNI_MAPPING = {\n",
    "    \"lni37\": 2003,\n",
    "    \"lni52\": 2004,\n",
    "    \"lni66\": 2005,\n",
    "    \"lni87\": 2006,\n",
    "    \"lni111\": 2007,\n",
    "    \"lni132\": 2008,\n",
    "    \"lni153\": 2009,\n",
    "    \"lni169\": 2010,\n",
    "    \"lni188\": 2011,\n",
    "    \"lni207\": 2012,\n",
    "    \"lni218\": 2013,\n",
    "    \"lni233\": 2014,\n",
    "    \"lni247\": 2015,\n",
    "    \"lni262\": 2016,\n",
    "    \"lni273\": 2017,\n",
    "    \"lni284\": 2018,\n",
    "    \"lni297\": 2019,\n",
    "    \"lni308\": 2020,\n",
    "    \"lni316\": 2021,\n",
    "    \"lni322\": 2022,\n",
    "    \"lni338\": 2023,\n",
    "    \"lni356\": 2024,\n",
    "    \"lni369\": 2025,\n",
    "}\n",
    "\n",
    "# Full proceedings PDFs to exclude (manually identified)\n",
    "FULL_PROCEEDINGS_SET = {\n",
    "    DATA_DIR / \"lni153/lni-p-153-komplett.pdf\",\n",
    "    DATA_DIR / \"lni169/lni-p-169-komplett.pdf\",\n",
    "    DATA_DIR / \"lni188/lni-p-188-komplett.pdf\",\n",
    "    DATA_DIR / \"lni207/lni-p-207-komplett.pdf\",\n",
    "    DATA_DIR / \"lni218/lni-p-218-komplett.pdf\",\n",
    "    DATA_DIR / \"lni233/lni-p-233-komplett.pdf\",\n",
    "    DATA_DIR / \"lni247/lni-p-247-komplett.pdf\",\n",
    "    DATA_DIR / \"lni262/lni-p-262-komplett.pdf\",\n",
    "    DATA_DIR / \"lni273/lni-p-273-komplett.pdf\",\n",
    "    DATA_DIR / \"lni284/proceedings_complete.pdf\",\n",
    "    DATA_DIR / \"lni297/DELFI2019_Tagungsband_komplett.pdf\",\n",
    "    DATA_DIR / \"lni297/DELFI2019_Tagungsband_komplett_Onlineversion.pdf\",\n",
    "    DATA_DIR / \"lni308/DELFI2020_Proceedings_komplett.pdf\",\n",
    "    DATA_DIR / \"lni316/DELFI_2021-Proceedings.pdf\",\n",
    "    DATA_DIR / \"lni322/DELFI_2022_Proceedings_FINAL.pdf\",\n",
    "    DATA_DIR / \"lni338/Komplettband.pdf\",\n",
    "    DATA_DIR / \"lni356/DELFI_2024_ProceedingsComplete_alt.pdf\",\n",
    "    DATA_DIR / \"lni356/DELFI_2024_ProceedingsComplete.pdf\",\n",
    "    DATA_DIR / \"lni356/proceedings.pdf\",\n",
    "    DATA_DIR / \"lni369/DELFI2025_ProceedingsComplete.pdf\",\n",
    "}\n",
    "\n",
    "# Terms to exclude (covers, prefaces, etc.)\n",
    "EXCLUSION_TERMS = [\"cover\", \"vorwort\", \"preface\", \"foreword\"]\n",
    "\n",
    "# Processing settings\n",
    "MIN_PAGES = 3  # Only process papers with > 2 pages\n",
    "\n",
    "print(f\"Full proceedings to exclude: {len(FULL_PROCEEDINGS_SET)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0625abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ parse_page_range tests passed\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2) Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def should_exclude_pdf(pdf_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Check if PDF should be excluded from annotation.\n",
    "    \n",
    "    Excludes:\n",
    "    - Full proceedings (from manual set)\n",
    "    - Covers, prefaces, forewords (by filename keyword)\n",
    "    \n",
    "    Returns:\n",
    "        True if PDF should be excluded, False otherwise\n",
    "    \"\"\"\n",
    "    # Check manual exclusion set\n",
    "    if pdf_path in FULL_PROCEEDINGS_SET:\n",
    "        return True\n",
    "    \n",
    "    # Check filename keywords (case-insensitive)\n",
    "    filename_lower = pdf_path.name.lower()\n",
    "    if any(term in filename_lower for term in EXCLUSION_TERMS):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def parse_page_range(page_str) -> tuple[int | None, int | None]:\n",
    "    \"\"\"\n",
    "    Parse 'start-end' page range format into (start_page, end_page).\n",
    "    \n",
    "    Handles mci.reference.pages format like '101-112'.\n",
    "    \n",
    "    Args:\n",
    "        page_str: String in format 'start-end' or None/NaN\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (start_page, end_page) as integers, or (None, None) if parsing fails\n",
    "    \"\"\"\n",
    "    if pd.isna(page_str) or not page_str:\n",
    "        return None, None\n",
    "    \n",
    "    parts = str(page_str).strip().split('-')\n",
    "    if len(parts) == 2:\n",
    "        try:\n",
    "            return int(parts[0].strip()), int(parts[1].strip())\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def load_metadata_excel(metadata_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load metadata Excel file with special handling for DeLFI2018.\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to metadata Excel file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with metadata\n",
    "    \"\"\"\n",
    "    if 'DeLFI2018' in metadata_path.name:\n",
    "        return pd.read_excel(metadata_path, sheet_name='Einreichung_GI') # the first sheet \"Arbeitsversion\" contains dirty data\n",
    "    else:\n",
    "        return pd.read_excel(metadata_path)\n",
    "    \n",
    "\n",
    "# Quick test of parse_page_range\n",
    "assert parse_page_range(\"101-112\") == (101, 112)\n",
    "assert parse_page_range(\"1-10\") == (1, 10)\n",
    "assert parse_page_range(None) == (None, None)\n",
    "assert parse_page_range(\"\") == (None, None)\n",
    "print(\"\\n✓ parse_page_range tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004628b",
   "metadata": {},
   "source": [
    "### 1) Pdf files with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955ee984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 metadata files:\n",
      "\n",
      "  lni132: metadata-lni-132.xlsx (39 rows)\n",
      "  lni153: metadata-lni-153.xlsx (25 rows)\n",
      "  lni188: metadata-lni-188.xlsx (23 rows)\n",
      "  lni207: metadata-lni-207.xlsx (31 rows)\n",
      "  lni218: metadata-lni-218.xlsx (34 rows)\n",
      "  lni284: metadata-DeLFI2018.xlsx (47 rows)\n",
      "  lni297: metadata-lni-297.xlsx (60 rows)\n",
      "  lni308: metadata-delfi-2020.xlsx (64 rows)\n",
      "  lni316: metadata-lni-316.xlsx (67 rows)\n",
      "  lni322: metadata_lni-322.xlsx (50 rows)\n",
      "  lni338: metadata_lni-338.xlsx (67 rows)\n",
      "  lni356: metadata_lni-356.xlsx (58 rows)\n",
      "  lni369: metadata_lni-369.xlsx (61 rows)\n",
      "  lni37: metadata-lni-37.xlsx (52 rows)\n",
      "  lni52: metadata-lni-52.xlsx (57 rows)\n",
      "  lni66: metadata-lni-66.xlsx (55 rows)\n",
      "  lni87: metadata-lni-87.xlsx (44 rows)\n",
      "\n",
      "============================================================\n",
      "Total LNI folders with metadata: 17\n",
      "Total papers in metadata: 834\n",
      "\n",
      "============================================================\n",
      "Column comparison (first vs last):\n",
      "\n",
      "lni132 columns (20):\n",
      "  ['dc.title', 'dc.contributor.author', 'dc.language.iso', 'dc.relation.ispartof', 'dc.contributor.editor', 'mci.reference.pages', 'dc.description.abstract', 'dc.subject', 'filename', 'dc.identifier.doi', 'dc.identifier.isbn', 'dc.identifier.issn', 'dc.relation.ispartofseries', 'dc.publisher', 'dc.pubPlace', 'dc.date.issued', 'mci.conference.date', 'mci.conference.location', 'mci.conference.sessiontitle', 'dc.type']\n",
      "\n",
      "lni87 columns (19):\n",
      "  ['dc.title', 'dc.contributor.author', 'dc.language.iso', 'dc.relation.ispartof', 'dc.contributor.editor', 'mci.reference.pages', 'dc.description.abstract', 'dc.subject', 'filename', 'dc.identifier.isbn', 'dc.identifier.issn', 'dc.relation.ispartofseries', 'dc.publisher', 'dc.pubPlace', 'dc.date.issued', 'mci.conference.date', 'mci.conference.location', 'mci.conference.sessiontitle', 'dc.type']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3) Discover and Load Metadata Files\n",
    "# =============================================================================\n",
    "\n",
    "# Find all metadata Excel files\n",
    "metadata_files = sorted(DATA_DIR.glob(\"*/metadata*.xlsx\"))\n",
    "\n",
    "print(f\"Found {len(metadata_files)} metadata files:\\n\")\n",
    "\n",
    "# Load each metadata file into a dictionary: {lni_folder: DataFrame}\n",
    "metadata_dict = {}\n",
    "\n",
    "for metadata_path in metadata_files:\n",
    "    lni_folder = metadata_path.parent.name\n",
    "    df = load_metadata_excel(metadata_path)\n",
    "    metadata_dict[lni_folder] = {\n",
    "        'path': metadata_path,\n",
    "        'df': df,\n",
    "        'n_rows': len(df)\n",
    "    }\n",
    "    print(f\"  {lni_folder}: {metadata_path.name} ({len(df)} rows)\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total LNI folders with metadata: {len(metadata_dict)}\")\n",
    "print(f\"Total papers in metadata: {sum(m['n_rows'] for m in metadata_dict.values())}\")\n",
    "\n",
    "# Quick inspection: show columns from first and last file to see variation\n",
    "first_lni = list(metadata_dict.keys())[0]\n",
    "last_lni = list(metadata_dict.keys())[-1]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Column comparison (first vs last):\")\n",
    "print(f\"\\n{first_lni} columns ({len(metadata_dict[first_lni]['df'].columns)}):\")\n",
    "print(f\"  {list(metadata_dict[first_lni]['df'].columns)}\")\n",
    "print(f\"\\n{last_lni} columns ({len(metadata_dict[last_lni]['df'].columns)}):\")\n",
    "print(f\"  {list(metadata_dict[last_lni]['df'].columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32c38f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized metadata for 17 LNI folders\n",
      "Total rows: 834\n",
      "\n",
      "Columns (22):\n",
      "['title', 'authors', 'language', 'proceeding_title', 'editors', 'abstract', 'subject', 'filename', 'doi', 'isbn', 'issn', 'series_title', 'publisher', 'publication_place', 'year', 'publication_type', 'conference_date', 'conference_location', 'session_title', 'start_page', 'end_page', 'peer_review_status']\n",
      "\n",
      "Sample row (first paper):\n",
      "{'title': 'Gibt es eine Net Generation? Widerlegung einer Mystifizierung', 'authors': 'Schulmeister, Rolf', 'language': 'de', 'proceeding_title': 'DeLFI 2008: Die 6. e-Learning Fachtagung Informatik', 'editors': 'Seehusen, Silke; Lucke, Ulrike; Fischer, Stefan', 'abstract': 'Es ist von einer Net Generation, von der Generation @, der Generation Y oder den Millenials die Rede, und es werden Mutmaßungen über die Rolle der Net Generation für die Lehre angestellt. Der Beitrag ist als kritische Analyse solcher Behauptungen und Mutmaßungen zu verstehen und als Diskurs zur Medien- nutzung aus der Sozialisationsperspektive.', 'subject': nan, 'filename': '15.pdf', 'doi': nan, 'isbn': '978-3-88579-226-0', 'issn': '1617-5468', 'series_title': 'Lecture Notes in Informatics (LNI) - Proceedings, Volume P-132', 'publisher': 'Gesellschaft für Informatik e. V.', 'publication_place': 'Bonn', 'year': 2008.0, 'publication_type': 'Text/Conference Paper', 'conference_date': '07. - 10. September 2008', 'conference_location': 'Lübeck', 'session_title': 'Regular Research Papers', 'start_page': 15.0, 'end_page': 28.0, 'peer_review_status': None}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4) Normalize Metadata Columns\n",
    "# =============================================================================\n",
    "\n",
    "# Column mapping: source column(s) -> MySQL column name\n",
    "# For columns with variants (mci.* vs gi.*), we list alternatives\n",
    "COLUMN_MAPPING = {\n",
    "    # Direct mappings (same in all files)\n",
    "    'dc.title': 'title',\n",
    "    'dc.contributor.author': 'authors',\n",
    "    'dc.language.iso': 'language',\n",
    "    'dc.relation.ispartof': 'proceeding_title',\n",
    "    'dc.contributor.editor': 'editors',\n",
    "    'dc.description.abstract': 'abstract',\n",
    "    'dc.subject': 'subject',\n",
    "    'filename': 'filename',\n",
    "    'dc.identifier.doi': 'doi',\n",
    "    'dc.identifier.isbn': 'isbn',\n",
    "    'dc.identifier.issn': 'issn',\n",
    "    'dc.relation.ispartofseries': 'series_title',\n",
    "    'dc.publisher': 'publisher',\n",
    "    'dc.pubPlace': 'publication_place',\n",
    "    'dc.date.issued': 'year',\n",
    "    'dc.type': 'publication_type',\n",
    "}\n",
    "\n",
    "# Columns with mci.* / gi.* variants\n",
    "VARIANT_COLUMNS = {\n",
    "    'conference_date': ['mci.conference.date', 'gi.conference.date'],\n",
    "    'conference_location': ['mci.conference.location', 'gi.conference.location'],\n",
    "    'session_title': ['mci.conference.sessiontitle', 'gi.conference.sessiontitle'],\n",
    "}\n",
    "\n",
    "# Special columns (require parsing or only in some files)\n",
    "# - mci.reference.pages -> start_page, end_page (15 of 17 files)\n",
    "# - gi.citation.startPage, gi.citation.endPage (2 of 17 files)\n",
    "# - gi.conference.review -> peer_review_status (only in gi.* files)\n",
    "\n",
    "\n",
    "def normalize_metadata_df(df: pd.DataFrame, lni_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize a metadata DataFrame to match MySQL column names.\n",
    "    \n",
    "    Handles:\n",
    "    - Direct column renames\n",
    "    - mci.* vs gi.* variants\n",
    "    - Page range parsing (mci.reference.pages -> start_page, end_page)\n",
    "    - gi.citation.startPage/endPage handling\n",
    "    \n",
    "    Args:\n",
    "        df: Raw metadata DataFrame\n",
    "        lni_folder: LNI folder name (for logging)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized DataFrame with MySQL column names\n",
    "    \"\"\"\n",
    "    normalized = pd.DataFrame()\n",
    "    \n",
    "    # 1) Direct mappings\n",
    "    for src_col, mysql_col in COLUMN_MAPPING.items():\n",
    "        if src_col in df.columns:\n",
    "            normalized[mysql_col] = df[src_col]\n",
    "        else:\n",
    "            normalized[mysql_col] = None\n",
    "    \n",
    "    # 2) Variant columns (mci.* / gi.*)\n",
    "    for mysql_col, variants in VARIANT_COLUMNS.items():\n",
    "        for variant in variants:\n",
    "            if variant in df.columns:\n",
    "                normalized[mysql_col] = df[variant]\n",
    "                break\n",
    "        else:\n",
    "            normalized[mysql_col] = None\n",
    "    \n",
    "    # 3) Page columns - handle both formats\n",
    "    if 'mci.reference.pages' in df.columns:\n",
    "        # Parse \"101-112\" format\n",
    "        pages = df['mci.reference.pages'].apply(parse_page_range)\n",
    "        normalized['start_page'] = pages.apply(lambda x: x[0])\n",
    "        normalized['end_page'] = pages.apply(lambda x: x[1])\n",
    "    elif 'gi.citation.startPage' in df.columns and 'gi.citation.endPage' in df.columns:\n",
    "        # Direct columns (convert to int, handling NaN)\n",
    "        normalized['start_page'] = pd.to_numeric(df['gi.citation.startPage'], errors='coerce').astype('Int64')\n",
    "        normalized['end_page'] = pd.to_numeric(df['gi.citation.endPage'], errors='coerce').astype('Int64')\n",
    "    else:\n",
    "        normalized['start_page'] = None\n",
    "        normalized['end_page'] = None\n",
    "    \n",
    "    # 4) Peer review status (only in gi.* files)\n",
    "    if 'gi.conference.review' in df.columns:\n",
    "        normalized['peer_review_status'] = df['gi.conference.review']\n",
    "    else:\n",
    "        normalized['peer_review_status'] = None\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "\n",
    "# Apply normalization to all metadata DataFrames\n",
    "normalized_metadata = {}\n",
    "all_normalized_dfs = []\n",
    "\n",
    "for lni_folder, data in metadata_dict.items():\n",
    "    df_normalized = normalize_metadata_df(data['df'], lni_folder)\n",
    "    normalized_metadata[lni_folder] = df_normalized\n",
    "    all_normalized_dfs.append(df_normalized)\n",
    "    \n",
    "# Combine all into one DataFrame\n",
    "df_all_metadata = pd.concat(all_normalized_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"Normalized metadata for {len(normalized_metadata)} LNI folders\")\n",
    "print(f\"Total rows: {len(df_all_metadata)}\")\n",
    "print(f\"\\nColumns ({len(df_all_metadata.columns)}):\")\n",
    "print(df_all_metadata.columns.tolist())\n",
    "\n",
    "# Quick check: show sample row\n",
    "print(f\"\\nSample row (first paper):\")\n",
    "print(df_all_metadata.iloc[0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca63d1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MySQL COLUMN VERIFICATION\n",
      "============================================================\n",
      "Required columns: 22\n",
      "Present in DataFrame: 22\n",
      "\n",
      "✓ All required columns present!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5) Verify all MySQL columns are present (except id, text, references)\n",
    "# =============================================================================\n",
    "\n",
    "MYSQL_COLUMNS = [\n",
    "    # 'id',  # AUTO_INCREMENT - generated by MySQL\n",
    "    'title',\n",
    "    'authors', \n",
    "    'year',\n",
    "    'abstract',\n",
    "    # 'text',  # Extracted from PDF\n",
    "    # 'references',  # Extracted from PDF\n",
    "    'start_page',\n",
    "    'end_page',\n",
    "    'subject',\n",
    "    'filename',\n",
    "    'editors',\n",
    "    'doi',\n",
    "    'isbn',\n",
    "    'issn',\n",
    "    'proceeding_title',\n",
    "    'series_title',\n",
    "    'publisher',\n",
    "    'publication_place',\n",
    "    'conference_date',\n",
    "    'conference_location',\n",
    "    'session_title',\n",
    "    'publication_type',\n",
    "    'language',\n",
    "    'peer_review_status',\n",
    "]\n",
    "\n",
    "# Check which columns are present/missing\n",
    "df_columns = set(df_all_metadata.columns)\n",
    "required_columns = set(MYSQL_COLUMNS)\n",
    "\n",
    "present = required_columns & df_columns\n",
    "missing = required_columns - df_columns\n",
    "extra = df_columns - required_columns\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MySQL COLUMN VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Required columns: {len(MYSQL_COLUMNS)}\")\n",
    "print(f\"Present in DataFrame: {len(present)}\")\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\n❌ MISSING columns ({len(missing)}):\")\n",
    "    for col in sorted(missing):\n",
    "        print(f\"   - {col}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All required columns present!\")\n",
    "\n",
    "if extra:\n",
    "    print(f\"\\nExtra columns (not in MySQL schema): {sorted(extra)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad34b1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PDF-METADATA MATCHING SUMMARY\n",
      "============================================================\n",
      "Successfully matched: 815 papers\n",
      "Unmatched metadata entries: 7\n",
      "PDFs without metadata (non-excluded): 6\n",
      "\n",
      "⚠️  Metadata entries without matching PDF:\n",
      "   ('lni132', 38, 'NaN filename')\n",
      "   ('lni284', 'Proceedings_complete.pdf', 'PDF not found')\n",
      "   ('lni37', 51, 'NaN filename')\n",
      "   ('lni52', 'GI.-.Proceedings.52-51.pdf', 'PDF not found')\n",
      "   ('lni52', 56, 'NaN filename')\n",
      "   ('lni66', 54, 'NaN filename')\n",
      "   ('lni87', 43, 'NaN filename')\n",
      "\n",
      "⚠️  PDFs without metadata (will be processed separately):\n",
      "   ('lni132', '433.pdf')\n",
      "   ('lni316', 'DELFI_2021_375-376.pdf')\n",
      "   ('lni316', 'DELFI_2021_23-24.pdf')\n",
      "   ('lni316', 'DELFI_2021_1-14.pdf')\n",
      "   ('lni316', 'DELFI_2021_349-350.pdf')\n",
      "   ('lni316', 'DELFI_2021_15-16.pdf')\n",
      "\n",
      "============================================================\n",
      "DISTRIBUTION BY YEAR\n",
      "============================================================\n",
      "  2003: 51 papers\n",
      "  2004: 55 papers\n",
      "  2005: 54 papers\n",
      "  2006: 43 papers\n",
      "  2008.0: 38 papers\n",
      "  2009: 24 papers\n",
      "  2011: 22 papers\n",
      "  2012: 30 papers\n",
      "  2013: 33 papers\n",
      "  2018: 46 papers\n",
      "  2019: 59 papers\n",
      "  2020: 62 papers\n",
      "  2021: 66 papers\n",
      "  2022: 49 papers\n",
      "  2023: 66 papers\n",
      "  nan: 1 papers\n",
      "  2024.0: 56 papers\n",
      "  2025: 60 papers\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6) Match PDFs with Metadata Entries\n",
    "# =============================================================================\n",
    "\n",
    "# For each LNI folder with metadata, find matching PDF files\n",
    "pdf_metadata_pairs = []  # List of (pdf_path, metadata_row) tuples\n",
    "unmatched_pdfs = []      # PDFs in folder but not in metadata\n",
    "unmatched_metadata = []  # Metadata entries without matching PDF\n",
    "\n",
    "for lni_folder, df_meta in normalized_metadata.items():\n",
    "    lni_path = DATA_DIR / lni_folder\n",
    "    \n",
    "    # Get all PDFs in this folder\n",
    "    pdf_files = {p.name: p for p in lni_path.glob(\"*.pdf\")}\n",
    "    \n",
    "    # Get filenames from metadata\n",
    "    metadata_filenames = set(df_meta['filename'].dropna().tolist())\n",
    "    \n",
    "    # Match each metadata entry to its PDF\n",
    "    for idx, row in df_meta.iterrows():\n",
    "        filename = row['filename']\n",
    "        \n",
    "        if pd.isna(filename):\n",
    "            unmatched_metadata.append((lni_folder, idx, \"NaN filename\"))\n",
    "            continue\n",
    "            \n",
    "        if filename in pdf_files:\n",
    "            pdf_path = pdf_files[filename]\n",
    "            \n",
    "            # Check exclusion BEFORE adding to pairs\n",
    "            if should_exclude_pdf(pdf_path):\n",
    "                continue  # Skip excluded PDFs\n",
    "                \n",
    "            pdf_metadata_pairs.append((pdf_path, row))\n",
    "        else:\n",
    "            unmatched_metadata.append((lni_folder, filename, \"PDF not found\"))\n",
    "    \n",
    "    # Find PDFs without metadata (for info only)\n",
    "    for pdf_name, pdf_path in pdf_files.items():\n",
    "        if pdf_name not in metadata_filenames:\n",
    "            if not should_exclude_pdf(pdf_path):  # Only report non-excluded\n",
    "                unmatched_pdfs.append((lni_folder, pdf_name))\n",
    "\n",
    "# Summary\n",
    "print(f\"{'='*60}\")\n",
    "print(\"PDF-METADATA MATCHING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully matched: {len(pdf_metadata_pairs)} papers\")\n",
    "print(f\"Unmatched metadata entries: {len(unmatched_metadata)}\")\n",
    "print(f\"PDFs without metadata (non-excluded): {len(unmatched_pdfs)}\")\n",
    "\n",
    "# Show details if there are issues\n",
    "if unmatched_metadata:\n",
    "    print(f\"\\n⚠️  Metadata entries without matching PDF:\")\n",
    "    for item in unmatched_metadata[:10]:  # Show first 10\n",
    "        print(f\"   {item}\")\n",
    "    if len(unmatched_metadata) > 10:\n",
    "        print(f\"   ... and {len(unmatched_metadata) - 10} more\")\n",
    "\n",
    "if unmatched_pdfs:\n",
    "    print(f\"\\n⚠️  PDFs without metadata (will be processed separately):\")\n",
    "    for item in unmatched_pdfs[:10]:  # Show first 10\n",
    "        print(f\"   {item}\")\n",
    "    if len(unmatched_pdfs) > 10:\n",
    "        print(f\"   ... and {len(unmatched_pdfs) - 10} more\")\n",
    "\n",
    "# Distribution by year\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DISTRIBUTION BY YEAR\")\n",
    "print(f\"{'='*60}\")\n",
    "year_counts = {}\n",
    "for pdf_path, row in pdf_metadata_pairs:\n",
    "    year = row['year']\n",
    "    year_counts[year] = year_counts.get(year, 0) + 1\n",
    "\n",
    "for year in sorted(year_counts.keys()):\n",
    "    print(f\"  {year}: {year_counts[year]} papers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0dc444",
   "metadata": {},
   "source": [
    "### 2) Pdf files without metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
